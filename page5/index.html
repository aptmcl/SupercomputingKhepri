<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <script src="/SupercomputingKhepri/libs/plotly/plotly.min.js"></script> <link rel=stylesheet  href="/SupercomputingKhepri/libs/highlight/github.min.css"> <link rel=stylesheet  href="/SupercomputingKhepri/css/franklin.css"> <link rel=stylesheet  href="/SupercomputingKhepri/css/tufte.css"> <link rel=stylesheet  href="/SupercomputingKhepri/css/latex.css"> <link rel=stylesheet  href="/SupercomputingKhepri/css/adjust.css"> <link rel=icon  href="/SupercomputingKhepri/assets/favicon.png"> <title>Rendering</title> <div id=layout > <div id=menu > <ul> <li><a href="/SupercomputingKhepri/">Home</a> <li><a href="/SupercomputingKhepri/page1/">Introduction</a> <li><a href="/SupercomputingKhepri/page2/">Parallel Processing</a> <li><a href="/SupercomputingKhepri/page3/">Design Exploration</a> <li><a href="/SupercomputingKhepri/page4/">Optimization</a> <li><a href="/SupercomputingKhepri/page5/">Rendering</a> <li><a href="/SupercomputingKhepri/page6/">Conclusions</a> </ul> </div> <div id=main > <div class=franklin-content > <h1 id=rendering ><a href="#rendering" class=header-anchor >Rendering</a></h1> <p><div class=franklin-toc ><ol><li><a href="#rendering_an_image">Rendering an Image</a><li><a href="#rendering_a_movie">Rendering a Movie</a><li><a href="#evolution">Evolution</a></ol></div> </p> <h1></h1> <p>Rendering is an extremely time-consuming task. At the same time, it is one that can have significant speedups when there are sufficient resources available. In this section, we experimented different rendering tasks and measured the effective gains.</p> <h3 id=rendering_an_image ><a href="#rendering_an_image" class=header-anchor >Rendering an Image</a></h3> <p>In this experiment, we tested the scalability of the popular raytracer POVRay. This is one of Khepri&#39;s rendering backends and, therefore, we used Khepri to generate the following 3D structure containing different materials &#40;metal, glass, etc.&#41;:</p> <img src="/SupercomputingKhepri/output/DomeTrussRibs.png" alt=""> <p>Usually, Khepri handles POVRay without any help from the user but, in this experiment, we did not want to include the time it takes Khepri to generate the information required for POVRay and then to start the process. Therefore, we took the input files generated by Khepri for POVRay and we tested them directly.</p> <p>We knew that, by default, POVRay uses all available CPUs to divide most of the raytracing process between them. However, we found it difficult to make it use fewer CPUs, even when we specified so on the batch script. We were able to solve the problem by using POVRay&#39;s <code>Work_Threads</code> option, which specifies the number of threads that it should use. The corresponding Slurm script looked like this:</p> <pre><code class="bash hljs"><span class=hljs-meta >#!/bin/bash</span>

<span class=hljs-comment >#SBATCH --job-name=TestPOVRay</span>
<span class=hljs-comment >#SBATCH --time=02:00:00</span>
<span class=hljs-comment >#SBATCH --nodes=1</span>
<span class=hljs-comment >#SBATCH --ntasks=1</span>
<span class=hljs-comment >#SBATCH -p &lt;...&gt;</span>
<span class=hljs-comment >#SBATCH -q &lt;...&gt;</span>

time povray Work_Threads=<span class=hljs-variable >$SLURM_CPUS_ON_NODE</span> DomeTrussRibs.ini</code></pre> <p>In this experiment, to avoid fluctuations in the load of the computing node, we decided to repeat the test three times and present the average. First, we attempted to render a 1024 by 768 image. The real time spent for different numbers of threads is the following:</p> <div id=fdpfgq  style="width:600px;height:350px"></div> <script> var fig = {"layout":{"showlegend":false,"xaxis":{"showticklabels":true,"gridwidth":0.5,"tickvals":[1.0,2.0,4.0,8.0,16.0,32.0,48.0,64.0,80.0,96.0],"visible":true,"ticks":"inside","range":[-1.85,98.85],"domain":[0.0,0.0],"tickmode":"array","linecolor":"rgba(0, 0, 0, 1.000)","showgrid":true,"title":"Threads","mirror":false,"tickangle":0,"showline":true,"gridcolor":"rgba(0, 0, 0, 0.100)","titlefont":{"color":"rgba(0, 0, 0, 1.000)","family":"sans-serif","size":15},"tickcolor":"rgb(0, 0, 0)","ticktext":["1","2","4","8","16","32","48","64","80","96"],"zeroline":false,"type":"-","tickfont":{"color":"rgba(0, 0, 0, 1.000)","family":"sans-serif","size":11},"zerolinecolor":"rgba(0, 0, 0, 1.000)","anchor":"y"},"paper_bgcolor":"rgba(255, 255, 255, 1.000)","annotations":[],"height":330,"margin":{"l":0,"b":20,"r":0,"t":20},"plot_bgcolor":"rgba(255, 255, 255, 1.000)","yaxis":{"showticklabels":true,"gridwidth":0.5,"tickvals":[0.0,200.0,400.0,600.0,800.0,1000.0,1200.0],"visible":true,"ticks":"inside","range":[-5.219409999999993,1255.4177433333332],"domain":[1.0,1.0],"tickmode":"array","linecolor":"rgba(0, 0, 0, 1.000)","showgrid":true,"title":"Time (s)","mirror":false,"tickangle":0,"showline":true,"gridcolor":"rgba(0, 0, 0, 0.100)","titlefont":{"color":"rgba(0, 0, 0, 1.000)","family":"sans-serif","size":15},"tickcolor":"rgb(0, 0, 0)","ticktext":["0","200","400","600","800","1000","1200"],"zeroline":false,"type":"-","tickfont":{"color":"rgba(0, 0, 0, 1.000)","family":"sans-serif","size":11},"zerolinecolor":"rgba(0, 0, 0, 1.000)","anchor":"x"},"width":640},"frames":[],"data":[{"xaxis":"x","colorbar":{"title":""},"yaxis":"y","x":[1.0,2.0,4.0,8.0,16.0,32.0,48.0,64.0,80.0,96.0],"showlegend":true,"mode":"lines+markers","name":"y1","zmin":null,"legendgroup":"y1","marker":{"symbol":"circle","color":"rgba(0, 154, 250, 1.000)","line":{"color":"rgba(0, 0, 0, 1.000)","width":1},"size":8},"zmax":null,"line":{"color":"rgba(0, 154, 250, 1.000)","width":1,"dash":"solid","shape":"linear"},"y":[1219.7393333333332,632.51825,364.2203333333334,213.85766666666666,115.14966666666665,61.57066666666666,45.0285,36.79900000000001,31.300666666666668,30.459],"type":"scatter"}]}; CONTAINER = document.getElementById('fdpfgq'); Plotly.newPlot(CONTAINER, fig.data, fig.layout) </script> <p>To have another data point, we then decided to change the point of view, while also increasing the size of the image from the previous 1024x768 to 1920x1024. This changes not only the number of pixels, but also the aspect ratio, producing the following image:</p> <img src="/SupercomputingKhepri/output/DomeTrussRibsFHD2.png" alt=""> <p>Again, we took the average of three different runs. The result is the following:</p> <div id=fdpffl  style="width:600px;height:350px"></div> <script> var fig = {"layout":{"showlegend":false,"xaxis":{"showticklabels":true,"gridwidth":0.5,"tickvals":[1.0,2.0,4.0,8.0,16.0,32.0,48.0,64.0,80.0,96.0],"visible":true,"ticks":"inside","range":[-1.85,98.85],"domain":[0.0,0.0],"tickmode":"array","linecolor":"rgba(0, 0, 0, 1.000)","showgrid":true,"title":"Threads","mirror":false,"tickangle":0,"showline":true,"gridcolor":"rgba(0, 0, 0, 0.100)","titlefont":{"color":"rgba(0, 0, 0, 1.000)","family":"sans-serif","size":15},"tickcolor":"rgb(0, 0, 0)","ticktext":["1","2","4","8","16","32","48","64","80","96"],"zeroline":false,"type":"-","tickfont":{"color":"rgba(0, 0, 0, 1.000)","family":"sans-serif","size":11},"zerolinecolor":"rgba(0, 0, 0, 1.000)","anchor":"y"},"paper_bgcolor":"rgba(255, 255, 255, 1.000)","annotations":[],"height":330,"margin":{"l":0,"b":20,"r":0,"t":20},"plot_bgcolor":"rgba(255, 255, 255, 1.000)","yaxis":{"showticklabels":true,"gridwidth":0.5,"tickvals":[0.0,2000.0,4000.0,6000.0],"visible":true,"ticks":"inside","range":[-101.1231733333333,7370.069173333332],"domain":[1.0,1.0],"tickmode":"array","linecolor":"rgba(0, 0, 0, 1.000)","showgrid":true,"title":"Time (s)","mirror":false,"tickangle":0,"showline":true,"gridcolor":"rgba(0, 0, 0, 0.100)","titlefont":{"color":"rgba(0, 0, 0, 1.000)","family":"sans-serif","size":15},"tickcolor":"rgb(0, 0, 0)","ticktext":["0","2000","4000","6000"],"zeroline":false,"type":"-","tickfont":{"color":"rgba(0, 0, 0, 1.000)","family":"sans-serif","size":11},"zerolinecolor":"rgba(0, 0, 0, 1.000)","anchor":"x"},"width":640},"frames":[],"data":[{"xaxis":"x","colorbar":{"title":""},"yaxis":"y","x":[1.0,2.0,4.0,8.0,16.0,32.0,48.0,64.0,80.0,96.0],"showlegend":true,"mode":"lines+markers","name":"y1","zmin":null,"legendgroup":"y1","marker":{"symbol":"circle","color":"rgba(0, 154, 250, 1.000)","line":{"color":"rgba(0, 0, 0, 1.000)","width":1},"size":8},"zmax":null,"line":{"color":"rgba(0, 154, 250, 1.000)","width":1,"dash":"solid","shape":"linear"},"y":[7158.620333333332,3055.4576666666667,1510.0136666666665,857.1473333333333,513.4073333333333,265.82300000000004,183.44333333333336,147.542,130.80100000000002,110.32566666666666],"type":"scatter"}]}; CONTAINER = document.getElementById('fdpffl'); Plotly.newPlot(CONTAINER, fig.data, fig.layout) </script> <p>Note that there are relevant speedups up to the upper limit of threads. Although it pales in comparison to the initial gains, from 80 threads to 96 threads, there is still a significant reduction from 130.8 seconds to 110.3 seconds.</p> <p>By analyzing the speedups, we can determine the number of threads that we should use.</p> <div id=fdptqk  style="width:600px;height:350px"></div> <script> var fig = {"layout":{"showlegend":true,"xaxis":{"showticklabels":true,"gridwidth":0.5,"tickvals":[1.0,2.0,4.0,8.0,16.0,32.0,48.0,64.0,80.0,96.0],"visible":true,"ticks":"inside","range":[-1.85,98.85],"domain":[0.0,0.0],"tickmode":"array","linecolor":"rgba(0, 0, 0, 1.000)","showgrid":true,"title":"Threads","mirror":false,"tickangle":0,"showline":true,"gridcolor":"rgba(0, 0, 0, 0.100)","titlefont":{"color":"rgba(0, 0, 0, 1.000)","family":"sans-serif","size":15},"tickcolor":"rgb(0, 0, 0)","ticktext":["1","2","4","8","16","32","48","64","80","96"],"zeroline":false,"type":"-","tickfont":{"color":"rgba(0, 0, 0, 1.000)","family":"sans-serif","size":11},"zerolinecolor":"rgba(0, 0, 0, 1.000)","anchor":"y"},"paper_bgcolor":"rgba(255, 255, 255, 1.000)","annotations":[],"height":330,"margin":{"l":0,"b":20,"r":0,"t":20},"plot_bgcolor":"rgba(255, 255, 255, 1.000)","yaxis":{"showticklabels":true,"gridwidth":0.5,"tickvals":[0.0,10.0,20.0,30.0,40.0,50.0,60.0],"visible":true,"ticks":"inside","range":[-0.9165879200065261,66.80285192022406],"domain":[1.0,1.0],"tickmode":"array","linecolor":"rgba(0, 0, 0, 1.000)","showgrid":true,"title":"Time (s)","mirror":false,"tickangle":0,"showline":true,"gridcolor":"rgba(0, 0, 0, 0.100)","titlefont":{"color":"rgba(0, 0, 0, 1.000)","family":"sans-serif","size":15},"tickcolor":"rgb(0, 0, 0)","ticktext":["0","10","20","30","40","50","60"],"zeroline":false,"type":"-","tickfont":{"color":"rgba(0, 0, 0, 1.000)","family":"sans-serif","size":11},"zerolinecolor":"rgba(0, 0, 0, 1.000)","anchor":"x"},"legend":{"borderwidth":1,"tracegroupgap":0,"font":{"color":"rgba(0, 0, 0, 1.000)","family":"sans-serif","size":11},"title":{"font":{"color":"rgba(0, 0, 0, 1.000)","family":"sans-serif","size":15},"text":""},"traceorder":"normal","x":0.07,"yanchor":"top","xanchor":"left","bordercolor":"rgba(0, 0, 0, 1.000)","bgcolor":"rgba(255, 255, 255, 1.000)","y":1.0},"width":640},"frames":[],"data":[{"xaxis":"x","colorbar":{"title":""},"yaxis":"y","x":[1.0,2.0,4.0,8.0,16.0,32.0,48.0,64.0,80.0,96.0],"showlegend":true,"mode":"lines+markers","name":"1024x768","zmin":null,"legendgroup":"1024x768","marker":{"symbol":"circle","color":"rgba(0, 154, 250, 1.000)","line":{"color":"rgba(0, 0, 0, 1.000)","width":1},"size":8},"zmax":null,"line":{"color":"rgba(0, 154, 250, 1.000)","width":1,"dash":"solid","shape":"linear"},"y":[1.0,1.92838599255173,3.3489051041448348,5.703509966909455,10.592643197693437,19.81039672571354,27.08816268215315,33.14599128599508,38.96847777470128,40.04528491852435],"type":"scatter"},{"xaxis":"x","colorbar":{"title":""},"yaxis":"y","x":[1.0,2.0,4.0,8.0,16.0,32.0,48.0,64.0,80.0,96.0],"showlegend":true,"mode":"lines+markers","name":"1920x1024","zmin":null,"legendgroup":"1920x1024","marker":{"symbol":"square","color":"rgba(227, 111, 71, 1.000)","line":{"color":"rgba(0, 0, 0, 1.000)","width":1},"size":8},"zmax":null,"line":{"color":"rgba(227, 111, 71, 1.000)","width":1,"dash":"solid","shape":"linear"},"y":[1.0,2.3428962578764794,4.740765260181972,8.351680107892768,13.94335426970917,26.93002612013758,39.02360583649809,48.519203571412426,54.72909483362766,64.88626400021754],"type":"scatter"}]}; CONTAINER = document.getElementById('fdptqk'); Plotly.newPlot(CONTAINER, fig.data, fig.layout) </script> <p>As is visible, for the small rendering task, it only pays off to use up to 80 threads, for an almost 40X speedup compared to just one thread. After that, the gains appear to be marginal. In the case of the large rendering task, despite the fluctuations, we were able to reach a speedup of almost 65 and the trend line shows that there are even bigger potential speedups waiting for us. In fact, POVRay can take advantage of 512 threads, so we are still a long way from that limit.</p> <h3 id=rendering_a_movie ><a href="#rendering_a_movie" class=header-anchor >Rendering a Movie</a></h3> <p>A movie is made of a sequence of images and, therefore, rendering a movie implies rendering multiples images. For smooth visualization, we should use a minimum of 30 frames per second, which means that a short 10-seconds movie requires at least 300 rendered images. If we further assume that the images should be in Full HD, i.e., using 1920x1080 pixels, then it becomes obvious that even a short movie can take a huge amount of time on a normal computer. In the recent past, we did several of these movies and it was not unusual to wait days or weeks for the completion of the rendering process.</p> <p>As we saw in the previous section, using the Cirrus supercomputer the time needed to render each frame becomes acceptable and thus, it is tempting to just generate all of the needed frames in sequence. This relieves the programmer from having to coordinate multiple processes. The following study of daylight, made of 157 frames at a resolution of 1024x768, was entirely done in 79m36s:</p> <video width=700  controls> <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/DomeTrussRibsDay-film.mp4" type="video/mp4"> Your browser does not support the video tag. </video> <p>Given that the speedup gets bigger at higher resolutions, we attempted the same experiment but now using 1920x1080 pixels and for a smoother effect, 391 frames:</p> <video width=700  controls> <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/DomeTrussRibsDayFHD-film.mp4" type="video/mp4"> Your browser does not support the video tag. </video> <p>This time, it took 797m30s, fitting the typical overnight rendering job.</p> <p>We saw in the previous experiment that POVRay will explore all available threads to render just one frame and, so, there are no more computational resources available that we can use to further speedup the process. If we start more POVRay processes on the computing node, the computing resources will be divided among them and, therefore, we will slowdown all of them. However, the Cirrus supercomputer has multiple computing nodes. This means that, although it might not be possible to speedup up the rendering of one image beyond the 96 threads available in one computing node, it is possible to speedup the rendering of a sequence of images by dividing the sequence among all available computing nodes. In our case, we were allowed to use four computing nodes and, although we did not experiment it because there were other jobs running that made it impossible to reserve all computing nodes for ourselves, it is clear that it would be possible to divide the rendering tasks among the four different computing nodes to achieve a further 4X speedup, allowing the rendering of a Full HD movie to achieve a speedup of 250.</p> <p>An easier experiment to do is the production of different movies. In this case, there is no need to coordinate the different computing nodes as each one can do a completely separate job. To prove this, we did a study on the different turbidity degrees of the atmosphere. First, we wrote a small Khepri script that would receive the turbidity degree as a command line argument and would generate a 400-frames movie of an animated object that is being viewed by a camera that rotates around it:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> KhepriPOVRay

turbidity=parse(<span class=hljs-built_in >Int</span>, <span class=hljs-literal >ARGS</span>[<span class=hljs-number >1</span>])
realistic_sky(turbidity=turbidity)

render_dir(<span class=hljs-meta >@__DIR__</span>)
render_size(<span class=hljs-number >1920</span>, <span class=hljs-number >1024</span>)

chrome = material(povray=&gt;povray_include(<span class=hljs-string >&quot;textures.inc&quot;</span>, <span class=hljs-string >&quot;texture&quot;</span>, <span class=hljs-string >&quot;Polished_Chrome&quot;</span>))
ground(-<span class=hljs-number >5</span>)

start_film(<span class=hljs-string >&quot;MetalGlassTree<span class=hljs-subst >$(turbidity)</span>&quot;</span>)
nframes = <span class=hljs-number >400</span>
<span class=hljs-keyword >for</span> (rho, phi, z) <span class=hljs-keyword >in</span> zip(division(<span class=hljs-number >80</span>, <span class=hljs-number >10</span>, nframes),division(<span class=hljs-number >0</span>, <span class=hljs-number >2</span>*<span class=hljs-literal >pi</span>, nframes),division(<span class=hljs-number >0</span>, <span class=hljs-number >40</span>, nframes))
 delete_all_shapes()
 <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-number >0</span>:<span class=hljs-number >19</span>
   p0 = cyl(<span class=hljs-number >23</span> - i, i, <span class=hljs-number >2</span>*i - <span class=hljs-number >2</span>*sin(<span class=hljs-number >4</span>*phi))
   p1 = cyl(<span class=hljs-number >23</span> - i, i + <span class=hljs-literal >pi</span>, <span class=hljs-number >2</span>*i + <span class=hljs-number >2</span>*sin(<span class=hljs-number >4</span>*phi))
   sphere(p0, <span class=hljs-number >2</span> + <span class=hljs-number >0.5</span>*sin(<span class=hljs-number >4</span>*phi), material=chrome)
   sphere(p1, <span class=hljs-number >2</span> - <span class=hljs-number >0.5</span>*sin(<span class=hljs-number >4</span>*phi), material=chrome)
   cylinder(p0, <span class=hljs-number >1</span>, p1, material=material_glass)
 <span class=hljs-keyword >end</span>
 set_view(cyl(rho, phi, z), xyz(<span class=hljs-number >0</span>, <span class=hljs-number >0</span>, <span class=hljs-number >30</span>), <span class=hljs-number >20.0</span>)
 save_film_frame()
<span class=hljs-keyword >end</span></code></pre> <p>We tested the script using eight different turbidity levels. The following videos illustrate the same animated object rendered under a selection of such levels &#40;more specifically, 2, 4, 6, and 8&#41;:</p> <video width=700  controls> <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/MetalGlassTreeMultiple.mp4" type="video/mp4"> Your browser does not support the video tag. </video> <p>The videos for the eight turbidity levels took, respectively, 94m8.523s, 75m36.726s, 73m30.351s, 73m19.054s, 71m32.826s, 71m23.082s, 70m53.158s, and 70m28.809s. Using just one computing node would entail roughly 10 hours &#40;more exactly, 600m52.529s&#41; but, as we were spreading the different independent jobs among the four computing nodes that we could use, it took around 2h50m &#40;more exactly, 169m42.249s&#41;. This result could be improved if we could use more computing nodes: each video could have been generated in a different computing node, and the total computation would have taken around 1h30m &#40;more exactly, 94m8.523s&#41;.</p> <p>For a more architectonic example, here is a study on different materials applied to a building&#39;s façade:</p> <video width=700  controls> <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/CarmoD-film.mp4" type="video/mp4"> Your browser does not support the video tag. </video> <video width=700  controls> <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/CarmoE-film.mp4" type="video/mp4"> Your browser does not support the video tag. </video> <video width=700  controls> <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/CarmoF-film.mp4" type="video/mp4"> Your browser does not support the video tag. </video> <p>The 360 frames in each of the three videos, in total, required 106 minutes to complete. However, by using three different nodes, we could have divided this time, roughly, by three.</p> <p>We also experimented with the renderings of glass in white or black backgrounds. To that end, we created the following Khepri program, which uses a ratio from 0.1 to 1.0 to affect the radius of each randomly placed sphere:</p> <pre><code class="julia hljs">default_material(material_glass)

ratio = Parameter(<span class=hljs-number >0.1</span>)

spheres_in_sphere(p, ri, re, rl, n) =
  <span class=hljs-keyword >if</span> n == <span class=hljs-number >0</span>
    <span class=hljs-literal >true</span>
  <span class=hljs-keyword >else</span>
    r = random_range(ri, re)
    sphere(p+vsph(r, random_range(<span class=hljs-number >0.0</span>, <span class=hljs-number >2</span>*<span class=hljs-literal >pi</span>), random_range(<span class=hljs-number >0.0</span>, <span class=hljs-literal >pi</span>)), (rl-r)*ratio())
    spheres_in_sphere(p, ri, re, rl, n-<span class=hljs-number >1</span>)
  <span class=hljs-keyword >end</span></code></pre> <p>To generate the frames, we just iterate, increasing the ratio on each frame, while rotating the spheres:</p> <pre><code class="julia hljs">ground(-<span class=hljs-number >6</span>, material(povray=&gt;povray_definition(
  <span class=hljs-string >&quot;Ground&quot;</span>, <span class=hljs-string >&quot;texture&quot;</span>,
  <span class=hljs-string >&quot;{ pigment { color rgb 1 } finish { reflection 0 ambient 0 }}&quot;</span>)))
realistic_sky()
render_size(<span class=hljs-number >1080</span>,<span class=hljs-number >1080</span>)
render_dir(<span class=hljs-meta >@__DIR__</span>)
set_view(xyz(<span class=hljs-number >9.9307</span>, -<span class=hljs-number >93.0178</span>, <span class=hljs-number >63.675</span>), xyz(<span class=hljs-number >0.0841</span>, -<span class=hljs-number >0.9705</span>, <span class=hljs-number >0.5236</span>), <span class=hljs-number >300</span>)

start_film(<span class=hljs-string >&quot;RotatingGrowingSpheres&quot;</span>)
<span class=hljs-keyword >for</span> ϕ <span class=hljs-keyword >in</span> division(<span class=hljs-number >0</span>, <span class=hljs-number >2</span><span class=hljs-literal >π</span>, <span class=hljs-number >720</span>)
  delete_all_shapes()
  set_random_seed(<span class=hljs-number >12345</span>)
  with(current_cs, cs_from_o_phi(u0(), ϕ), ratio, ϕ/(<span class=hljs-number >2</span><span class=hljs-literal >π</span>)) <span class=hljs-keyword >do</span>
    spheres_in_sphere(xyz(<span class=hljs-number >0</span>, <span class=hljs-number >0</span>, <span class=hljs-number >0</span>), <span class=hljs-number >4.0</span>, <span class=hljs-number >5.0</span>, <span class=hljs-number >5.0</span>, <span class=hljs-number >600</span>)
  <span class=hljs-keyword >end</span>
  save_film_frame()
<span class=hljs-keyword >end</span></code></pre> <p>Then just by changing the <code>rgb</code> color of the ground, we generate the two different backgrounds. The results are the following:</p> <video width=700  controls> <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/RotatingGrowingSpheres.mp4" type="video/mp4"> Your browser does not support the video tag. </video> <p>The film with white background took 218m15.151s while the one with the black background took 89m36.919s. This is one example where dividing the work among two computing nodes does not provide as much benefit as we would like because one of the videos takes much longer to produce than the other. Nevertheless, it still saves almost one and a half hour in a job that would take five hours to complete, a still significant 30&#37; reduction.</p> <p>Finally, given that the focus was architecture, we decided to repeat a series of videos that we did in the past, at a time where we spent weeks rendering films that, in some cases, would take one hour for each frame. Given the differences in the available software, it is not possible to exactly replicate the images, as the rendering engine is necessarily different. However, it can give a sense of the trade-offs between speed and image quality.</p> <p>The first video shows a parametric exploration of the Astana National Library, a project originally designed BIG architects &#40;the video was &#39;filmed&#39; at Full HD resolution but was reduced to half its size to facilitate viewing&#41;:</p> <video width=700  controls> <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/Astana_rubber-film.mp4" type="video/mp4"> Your browser does not support the video tag. </video> <p>For another example:</p> <video width=700  controls> <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/2_Tracking-Cyl.mp4" type="video/mp4"> Your browser does not support the video tag. </video> <h3 id=evolution ><a href="#evolution" class=header-anchor >Evolution</a></h3> <p>We saw that supercomputers can have a dramatic effect on the time needed for rendering tasks. By parallelizing the rendering of a single image through the multi-processing capabilities of a computing node and then parallelizing the rendering of multiple images through the use of multiple computing nodes, it becomes possible to achieve very large speedups.</p> <p>At the same time, it is relevant to consider that even commodity hardware, nowadays, can efficiently run multiple threads in parallel. This means that the speedups obtained in the previous experiments must be contrasted not with the minimum computing power that the supercomputer can provide but, instead, with the current computing power that is available almost everywhere. In this analysis, the results do not look as good as they seemed. As a reference, using the maximum computing power available on one computing node, i.e., 96 execution threads, we managed to render the 1920x1024 image in an average of 110.3 seconds. For comparison, a 2017 AMD ThreadRipper 1950X workstation providing 16 cores/32 threads renders that same image in 615.5 seconds, which represents a speedup of only 5.6. For an even more depressing comparison, a 2015 Intel 4 cores/8 threads i7-6700K CPU that costs around 250 EUR can render the same image in 1946.4 seconds while doing other useful tasks at the same time. Although the supercomputer gives us a speedup of 17.6, just the CPU costs 18 times more. The ratio cost/performance seems to be, at best, constant.</p> <p>Despite the cost, the supercomputer does make the rendering task more feasible. We decided to test some additional examples that, in the past, were almost impractical. As a first example, in 2010, the following image, by Prateek Karandikar, took 16 hours and 19 seconds to render on an Intel Pentium 1.8GHz machine with 1GB RAM.</p> <img src="/SupercomputingKhepri/output/Photon.png" alt=""> <p>The supercomputer could generate the same image in 51 seconds, which is more than three orders of magnitude faster.</p> <p>As another example, consider the classical POVRay Hall-of-Fame <em>Pebbles</em> example, which is entirely procedurally generated:</p> <img src="/SupercomputingKhepri/output/pebbles.png" alt=""> <p>According to its author, this image took 4.5 days to render on an Athlon 5600&#43;. We generated the exact same image on the supercomputer in 2h49m. This is a speedup of almost 40, which opens the door to other ideas. One was to use the exact same POVRay program to do a short movie just by changing the camera&#39;s position. The result is not very smooth but it gives an idea of what becomes possible:</p> <img src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/PebblesZoomInOutFilm.gif" alt=""> <h1></h1> <p><a href="/SupercomputingKhepri/page4/">&lt;&lt; Previous Chapter</a></p> <p><a href="/SupercomputingKhepri/page6/">Next Chapter &gt;&gt;</a></p> <div class=page-foot > <div class=copyright > &copy; António Menezes Leitão. Last modified: March 19, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div> </div>