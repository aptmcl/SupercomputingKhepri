@def title = "Rendering"

```julia:setup
#hideall
using Franklin
using DataFrames
using CSV
using Plots
using StatsPlots
using JSON
using Statistics: mean

plotlyjs(size=(640,330))

bench_data(str) =
  DataFrame(CSV.File(IOBuffer(str), delim=" ", ignorerepeated=true))

saveplot(plt, name="", ext="svg") =
  fdplotly(json(Plots.plotlyjs_syncplot(plt))) # hide
  #savefig(joinpath(@OUTPUT, name * "." * ext))
```

# Rendering

\tableofcontents <!-- you can use \toc as well -->

#

Rendering is an extremely time-consuming task. At the same time, it is
one that can have significant speedups when there are sufficient
resources available. In this section, we experimented different
rendering tasks and measured the effective gains.

### Rendering an Image

In this experiment, we tested the scalability of the popular raytracer
POVRay. This is one of Khepri's rendering backends and, therefore, we
used Khepri to generate the following 3D structure containing
different materials (metal, glass, etc.):

\fig{/DomeTrussRibs}

Usually, Khepri handles POVRay without any help from the user but, in
this experiment, we did not want to include the time it takes
Khepri to generate the information required for POVRay and then to start the process.
Therefore, we took the input files generated by Khepri for POVRay and
we tested them directly.

We knew that, by default, POVRay uses all available CPUs to divide
most of the raytracing process between them. However, we found it
difficult to make it use fewer CPUs, even when we specified so on the
batch script. We were able to solve the problem by using POVRay's
`Work_Threads` option, which specifies the number of threads
that it should use. The corresponding Slurm script looked like this:

```bash
#!/bin/bash

#SBATCH --job-name=TestPOVRay
#SBATCH --time=02:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH -p <...>
#SBATCH -q <...>

time povray Work_Threads=$SLURM_CPUS_ON_NODE DomeTrussRibs.ini
```

In this experiment, to avoid fluctuations in the load of the computing
node, we decided to repeat the test three times and present the
average. First, we attempted to render a 1024 by 768 image. The real
time spent for different numbers of threads is the following:

```julia:plot10
#hideall
povray_1024x768 = bench_data("""
RadiosityTime RadiosityThreads RadiosityTotal TraceTime TraceThreads TraceTotal RealTime UserTime SysTime
0.818 3 2.368 31.231 64 1941.229 0m36.337s 32m25.434s 0m0.635s
0.819 3 2.361 25.443 96 2107.544 0m30.455s 35m11.783s 0m0.637s
0.819 3 2.366 26.189 80 2017.256 0m31.304s 33m41.534s 0m0.597s
0.821 3 2.363 39.692 48 1840.427 0m44.731s 30m44.612s 0m0.600s
0.819 3 2.366 56.095 32 1774.080 1m1.169s 29m38.282s 0m0.568s
0.814 3 2.353 108.455 16 1729.514 1m53.464s 28m53.648s 0m0.626s
0.817 3 2.361 208.080 8 1659.078 3m33.054s 27m43.235s 0m0.624s
0.814 3 2.346 352.963 4 1410.929 5m57.999s 23m35.000s 0m0.666s
0.817 3 2.347 611.214 2 1221.676 10m16.227s 20m25.473s 0m0.964s
1.574 2 2.370 1207.878 1 1207.088 20m13.614s 20m11.462s 0m0.559s
1.572 2 2.370 1207.309 1 1206.513 20m13.098s 20m10.883s 0m0.571s
0.816 3 2.348 620.177 2 1239.598 10m25.139s 20m43.725s 0m0.642s
0.816 3 2.356 25.325 96 2126.921 0m30.392s 35m31.212s 0m0.591s
0.814 3 2.357 26.340 80 2027.735 0m31.387s 33m51.958s 0m0.585s
0.814 3 2.359 40.328 48 1872.833 0m45.326s 31m16.967s 0m0.659s
0.817 3 2.362 57.289 32 1816.192 1m2.315s 30m20.225s 0m0.734s
0.816 3 2.360 108.502 16 1729.631 1m53.534s 28m53.794s 0m0.599s
0.818 3 2.361 206.353 8 1648.052 3m31.356s 27m32.014s 0m0.749s
0.817 3 2.366 354.500 4 1417.245 5m59.522s 23m41.345s 0m0.618s
0.817 3 2.356 631.685 2 1262.454 10m36.675s 21m6.617s 0m0.757s
1.585 2 2.386 1226.822 1 1226.094 20m32.506s 20m30.473s 0m0.689s
0.816 3 2.354 647.005 2 1292.969 10m52.032s 21m36.416s 0m1.615s
0.813 3 2.350 370.097 4 1477.886 6m15.140s 24m41.825s 0m0.839s
0.816 3 2.357 212.190 8 1659.878 3m37.163s 27m44.064s 0m0.542s
0.820 3 2.372 113.396 16 1696.150 1m58.451s 28m20.363s 0m0.513s
0.824 3 2.371 56.236 32 1783.993 1m1.228s 29m48.213s 0m0.524s
0.816 3 2.363 32.138 64 1913.001 0m37.261s 31m57.256s 0m0.566s
0.814 3 2.357 26.156 80 2017.308 0m31.211s 33m41.559s 0m0.604s
0.823 3 2.373 25.497 96 2102.918 0m30.530s 35m7.262s 0m0.588s
""")

time2seconds(s) =
  let m = match(r"(.+)m(.+)s", s)
    parse(Float64, m.captures[1])*60+parse(Float64, m.captures[2])
  end

plot_povray(raw_data) =
  let data = sort(combine(groupby(raw_data, :TraceThreads),
                          :RealTime => it->mean(map(time2seconds, it))),
                  :TraceThreads)
    plot(data[:,1],
         data[:,2],
         xticks=data[:,1],
         legend=:none,
         markers=:auto,
         #ylimits=(0,180),
         xlabel="Threads",
         #color=:green,
         #xscale=:log10,
         ylabel="Time (s)")
  end

plt = plot_povray(povray_1024x768)
saveplot(plt,"POVRay1024x768")
```
\textoutput{plot10}

To have another data point, we then decided to change the point of
view, while also increasing the size of the image from the previous
1024x768 to 1920x1024. This changes not only the number of pixels, but
also the aspect ratio, producing the following image:

\fig{/DomeTrussRibsFHD2}

Again, we took the average of three different runs. The result is the following:

```julia:plot11
#hideall
povray_1920x1024 = bench_data("""
RadiosityTime RadiosityThreads RadiosityTotal TraceTime TraceThreads TraceTotal RealTime UserTime SysTime
0.779 3 2.001 102.021 96 9619.841 1m47.700s 160m24.887s 0m0.904s
0.762 3 1.985 118.116 80 9315.123 2m3.798s 155m20.004s 0m1.037s
0.789 3 2.017 137.704 64 8669.172 2m23.322s 144m33.953s 0m1.112s
0.778 3 1.996 255.177 32 8125.378 4m20.778s 135m30.414s 0m0.919s
0.768 3 1.991 176.663 48 8405.420 3m2.357s 140m10.271s 0m1.085s
0.764 3 1.972 514.804 16 8222.332 8m40.465s 137m7.376s 0m1.434s
0.779 3 1.996 778.487 8 6225.444 13m4.098s 103m50.300s 0m1.224s
0.764 3 1.972 1407.864 4 5628.796 23m33.505s 93m51.447s 0m3.593s
0.774 3 1.988 2808.148 2 5612.965 46m53.850s 93m38.552s 0m1.956s
1.372 2 2.001 5324.891 1 5321.549 88m51.048s 88m46.425s 0m3.231s
1.367 2 1.993 8705.511 1 8696.566 145m11.838s 145m3.143s 0m7.101s
0.778 3 1.996 3454.576 2 6902.697 57m40.354s 115m7.432s 0m5.056s
0.772 3 1.985 1500.587 4 5999.998 25m6.543s 100m5.298s 0m1.310s
0.779 3 1.993 955.979 8 7643.130 16m1.935s 127m28.370s 0m1.753s
0.777 3 2.007 521.439 16 8326.534 8m47.077s 138m51.595s 0m1.320s
0.779 3 1.999 272.784 32 8679.197 4m38.447s 144m41.701s 0m3.616s
0.777 3 1.995 175.634 48 8307.712 3m1.251s 138m32.662s 0m0.889s
0.782 3 2.001 142.765 64 8820.308 2m28.449s 147m5.241s 0m0.977s
0.772 3 1.983 120.288 80 9283.625 2m6.013s 154m48.632s 0m0.897s
0.787 3 2.024 105.754 96 9971.934 1m51.491s 166m17.003s 0m0.862s
1.515 2 2.194 7426.595 1 7418.852 123m52.975s 123m46.059s 0m4.959s
0.773 3 1.986 2886.633 2 5768.779 48m12.169s 96m14.905s 0m2.498s
0.774 3 1.986 1604.454 4 6408.280 26m49.993s 106m53.697s 0m1.581s
0.778 3 1.999 819.846 8 6550.907 13m45.409s 109m16.001s 0m1.120s
0.771 3 1.982 487.128 16 7772.255 8m12.680s 129m37.101s 0m1.092s
0.796 3 2.032 252.587 32 8035.035 4m18.244s 134m0.110s 0m0.851s
0.785 3 2.008 181.167 48 8443.545 3m6.722s 140m48.535s 0m0.895s
0.765 3 1.982 145.240 64 9035.470 2m30.855s 150m40.471s 0m0.908s
0.775 3 1.992 136.878 80 9359.387 2m22.592s 156m4.452s 0m0.878s
0.788 3 2.024 106.134 96 10010.192 1m51.786s 166m55.312s 0m0.851s
""")

plt = plot_povray(povray_1920x1024)
saveplot(plt,"POVRay1920x1024")
```
\textoutput{plot11}

Note that there are relevant speedups up to the upper limit of
threads. Although it pales in comparison to the initial gains, from 80
threads to 96 threads, there is still a significant reduction from
130.8 seconds to 110.3 seconds.

By analyzing the speedups, we can determine the number of threads that
we should use.

```julia:plot12
#hideall
plot_povray_speedup(raw_data1, raw_data2) =
  let process_data(raw_data) =
            sort(combine(groupby(raw_data, :TraceThreads),
                         :RealTime => it->mean(map(time2seconds, it))),
                 :TraceThreads),
      data1 = process_data(raw_data1),
      data2 = process_data(raw_data2)
    plot(data1[:,1],
         [map(x->data1[1,2]/x, data1[:,2]) map(x->data2[1,2]/x, data2[:,2])],
         xticks=data1[:,1],
         label=["1024x768" "1920x1024"],
         legend=:topleft,
         markers=:auto,
         #ylimits=(0,180),
         xlabel="Threads",
         #color=:green,
         #xscale=:log10,
         ylabel="Time (s)")
  end

average96 =
  combine(filter(:TraceThreads => x->x==96, povray_1920x1024),
          :RealTime => it->mean(map(time2seconds, it)))

plt = plot_povray_speedup(povray_1024x768, povray_1920x1024)
saveplot(plt,"POVRay1024x768vs1920x1024")
```
\textoutput{plot12}

As is visible, for the small rendering task, it only pays off to use
up to 80 threads, for an almost 40X speedup compared to just one
thread. After that, the gains appear to be marginal. In the case of
the large rendering task, despite the fluctuations, we were able to
reach a speedup of almost 65 and the trend line shows that there are
even bigger potential speedups waiting for us. In fact, POVRay can
take advantage of 512 threads, so we are still a long way from
that limit.

### Rendering a Movie

A movie is made of a sequence of images and, therefore, rendering a
movie implies rendering multiples images. For smooth visualization, we
should use a minimum of 30 frames per second, which means that a short
10-seconds movie requires at least 300 rendered images. If we further
assume that the images should be in Full HD, i.e., using 1920x1080
pixels, then it becomes obvious that even a short movie can take a
huge amount of time on a normal computer. In the recent past, we did
several of these movies and it was not unusual to wait days or weeks
for the completion of the rendering process.

As we saw in the previous section, using the Cirrus supercomputer the
time needed to render each frame becomes acceptable and thus, it is
tempting to just generate all of the needed frames in sequence. This
relieves the programmer from having to coordinate multiple processes.
The following study of daylight, made of 157 frames at a resolution of
1024x768, was entirely done in 79m36s:

~~~
<video width="700" controls>
  <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/DomeTrussRibsDay-film.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
~~~

Given that the speedup gets bigger at higher resolutions, we attempted the same experiment but now
using 1920x1080 pixels and for a smoother effect, 391 frames:

~~~
<video width="700" controls>
  <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/DomeTrussRibsDayFHD-film.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
~~~

This time, it took 797m30s, fitting the typical overnight rendering job.

We saw in the previous experiment that POVRay will explore all
available threads to render just one frame and, so, there are no more
computational resources available that we can use to further speedup
the process. If we start more POVRay processes on the computing node,
the computing resources will be divided among them and, therefore, we
will slowdown all of them. However, the Cirrus supercomputer has
multiple computing nodes. This means that, although it might not be
possible to speedup up the rendering of one image beyond the 96
threads available in one computing node, it is possible to speedup the
rendering of a sequence of images by dividing the sequence among all
available computing nodes. In our case, we were allowed to use four
computing nodes and, although we did not experiment it because there
were other jobs running that made it impossible to reserve all
computing nodes for ourselves, it is clear that it would be possible
to divide the rendering tasks among the four different computing nodes
to achieve a further 4X speedup, allowing the rendering of a Full HD
movie to achieve a speedup of 250.

An easier experiment to do is the production of different
movies. In this case, there is no need to coordinate the different
computing nodes as each one can do a completely separate job. To prove
this, we did a study on the different turbidity degrees of the
atmosphere. First, we wrote a small Khepri script that would receive
the turbidity degree as a command line argument and would generate a
400-frames movie of an animated object that is being viewed by a
camera that rotates around it:

```julia
using KhepriPOVRay

turbidity=parse(Int, ARGS[1])
realistic_sky(turbidity=turbidity)

render_dir(@__DIR__)
render_size(1920, 1024)

chrome = material(povray=>povray_include("textures.inc", "texture", "Polished_Chrome"))
ground(-5)

start_film("MetalGlassTree$(turbidity)")
nframes = 400
for (rho, phi, z) in zip(division(80, 10, nframes),division(0, 2*pi, nframes),division(0, 40, nframes))
 delete_all_shapes()
 for i in 0:19
   p0 = cyl(23 - i, i, 2*i - 2*sin(4*phi))
   p1 = cyl(23 - i, i + pi, 2*i + 2*sin(4*phi))
   sphere(p0, 2 + 0.5*sin(4*phi), material=chrome)
   sphere(p1, 2 - 0.5*sin(4*phi), material=chrome)
   cylinder(p0, 1, p1, material=material_glass)
 end
 set_view(cyl(rho, phi, z), xyz(0, 0, 30), 20.0)
 save_film_frame()
end
```

We tested the script using eight different turbidity levels. The
following videos illustrate the same animated object rendered under a
selection of such levels (more specifically, 2, 4, 6, and 8):

~~~
<video width="700" controls>
  <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/MetalGlassTreeMultiple.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
~~~

The videos for the eight turbidity levels took, respectively,
94m8.523s, 75m36.726s, 73m30.351s, 73m19.054s, 71m32.826s, 71m23.082s,
70m53.158s, and 70m28.809s. Using just one computing node would
entail roughly 10 hours (more exactly, 600m52.529s) but, as we were
spreading the different independent jobs among the four computing
nodes that we could use, it took around 2h50m (more exactly,
169m42.249s). This result could be improved if we could use more
computing nodes: each video could have been generated in a different
computing node, and the total computation would have taken around
1h30m (more exactly, 94m8.523s).

For a more architectonic example, here is a study on different materials applied to a building's façade:

~~~
<video width="700" controls>
  <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/CarmoD-film.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
<video width="700" controls>
  <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/CarmoE-film.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
<video width="700" controls>
  <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/CarmoF-film.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
~~~

The 360 frames in each of the three videos, in total, required 106
minutes to complete. However, by using three different nodes, we could
have divided this time, roughly, by three.

We also experimented with the renderings of glass in white or black
backgrounds. To that end, we created the following Khepri program,
which uses a ratio from 0.1 to 1.0 to affect the radius of each
randomly placed sphere:

```julia
default_material(material_glass)

ratio = Parameter(0.1)

spheres_in_sphere(p, ri, re, rl, n) =
  if n == 0
    true
  else
    r = random_range(ri, re)
    sphere(p+vsph(r, random_range(0.0, 2*pi), random_range(0.0, pi)), (rl-r)*ratio())
    spheres_in_sphere(p, ri, re, rl, n-1)
  end
```

To generate the frames, we just iterate, increasing the ratio on each
frame, while rotating the spheres:

```
ground(-6, material(povray=>povray_definition(
  "Ground", "texture",
  "{ pigment { color rgb 1 } finish { reflection 0 ambient 0 }}")))
realistic_sky()
render_size(1080,1080)
render_dir(@__DIR__)
set_view(xyz(9.9307, -93.0178, 63.675), xyz(0.0841, -0.9705, 0.5236), 300)

start_film("RotatingGrowingSpheres")
for ϕ in division(0, 2π, 720)
  delete_all_shapes()
  set_random_seed(12345)
  with(current_cs, cs_from_o_phi(u0(), ϕ), ratio, ϕ/(2π)) do
    spheres_in_sphere(xyz(0, 0, 0), 4.0, 5.0, 5.0, 600)
  end
  save_film_frame()
end
```

Then just by changing the `rgb` color of the ground, we generate the
two different backgrounds. The results are the following:

~~~
<video width="700" controls>
  <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/RotatingGrowingSpheres.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
~~~

The film with white background took 218m15.151s while the one with the
black background took 89m36.919s. This is one example where
dividing the work among two computing nodes does not provide as much
benefit as we would like because one of the videos takes much longer
to produce than the other. Nevertheless, it still saves almost one and
a half hour in a job that would take five hours to complete, a still
significant 30% reduction.

Finally, given that the focus was architecture, we decided to repeat a
series of videos that we did in the past, at a time where we spent
weeks rendering films that, in some cases, would take one hour for
each frame. Given the differences in the available software, it is not
possible to exactly replicate the images, as the rendering engine is
necessarily different. However, it can give a sense of the trade-offs
between speed and image quality.

The first video shows a parametric exploration of the Astana National Library, a project originally
designed BIG architects (the video was 'filmed' at Full HD resolution but was reduced to half its
size to facilitate viewing):

~~~
<video width="700" controls>
  <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/Astana_rubber-film.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
~~~

For another example:

~~~
<video width="700" controls>
  <source src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/2_Tracking-Cyl.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
~~~

### Evolution

We saw that supercomputers can have a dramatic effect on the time
needed for rendering tasks. By parallelizing the rendering of a
single image through the multi-processing capabilities of a computing
node and then parallelizing the rendering of multiple images through
the use of multiple computing nodes, it becomes possible to achieve
very large speedups.

At the same time, it is relevant to consider that even commodity
hardware, nowadays, can efficiently run multiple threads in
parallel. This means that the speedups obtained in the previous
experiments must be contrasted not with the minimum computing power
that the supercomputer can provide but, instead, with the current
computing power that is available almost everywhere. In this
analysis, the results do not look as good as they seemed. As a
reference, using the maximum computing power available on one
computing node, i.e., 96 execution threads, we managed to render the
1920x1024 image in an average of 110.3 seconds. For comparison, a 2017
AMD ThreadRipper 1950X workstation providing 16 cores/32 threads
renders that same image in 615.5 seconds, which represents a speedup
of only 5.6. For an even more depressing comparison, a 2015 Intel 4
cores/8 threads i7-6700K CPU that costs around 250 EUR can render the
same image in 1946.4 seconds while doing other useful tasks at the
same time. Although the supercomputer gives us a speedup of 17.6, just
the CPU costs 18 times more. The ratio cost/performance seems to be,
at best, constant.

Despite the cost, the supercomputer does make the rendering task more
feasible. We decided to test some additional examples that, in the
past, were almost impractical. As a first example, in 2010, the
following image, by Prateek Karandikar, took 16 hours and 19 seconds
to render on an Intel Pentium 1.8GHz machine with 1GB RAM.

\fig{/Photon}

The supercomputer could generate the same image in 51 seconds, which
is more than three orders of magnitude faster.

As another example, consider the classical POVRay Hall-of-Fame
_Pebbles_ example, which is entirely procedurally generated:

\fig{/pebbles}

According to its author, this image took 4.5 days to render on an
Athlon 5600+. We generated the exact same image on the supercomputer
in 2h49m. This is a speedup of almost 40, which opens the door to
other ideas. One was to use the exact same POVRay program to do a short
movie just by changing the camera's position. The result is not very smooth but
it gives an idea of what becomes possible:

~~~
<img src="http://web.ist.utl.pt/antonio.menezes.leitao/ADA/SuperComputingFilms/PebblesZoomInOutFilm.gif" alt="">
~~~

#
[<< Previous Chapter](/page4/)

[Next Chapter >>](/page6/)
